# æ¥å…¥ LangSmith & Deep Research å¢å¼ºç‰ˆ
from typing import List, Dict, Optional
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from openai import OpenAI
from langsmith.wrappers import wrap_openai
from langsmith import traceable
import uuid, os, json
from datetime import date
from concurrent.futures import ThreadPoolExecutor

# åˆå§‹åŒ–ç¯å¢ƒ
load_dotenv()
os.environ["LANGSMITH_PROJECT"] = "deep_research_tracing"
os.environ["LANGSMITH_TRACING"] = "true"

# åŒ…è£… OpenAI å®¢æˆ·ç«¯ä»¥æ”¯æŒ LangSmith è¿½è¸ª
openai_client = wrap_openai(OpenAI())

# LLM è°ƒç”¨
@traceable(name="LLM Call")
def llm_call(prompt: str, model: str="gpt-4o-mini", temperature: float=0.5) -> str:
    resp = openai_client.chat.completions.create(
        model=model,
        messages=[{"role":"user","content":prompt}],
        temperature=temperature
    )
    return resp.choices[0].message.content

# Memory / Embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

import hashlib

def get_vectorstore(topic: str) -> Chroma:
    # å°† topic è½¬æˆå“ˆå¸Œå€¼ï¼Œå®‰å…¨åš collection åç§°
    topic_hash = hashlib.md5(topic.encode('utf-8')).hexdigest()
    name = f"deep_research_memory_{topic_hash}"  # ç¬¦åˆ Chroma å‘½åè§„èŒƒ
    os.makedirs("./chroma_db", exist_ok=True)
    try:
        store = Chroma(
            collection_name=name,
            embedding_function=embeddings,
            persist_directory="./chroma_db"
        )
        if not getattr(store, "_collection", None):
            store.reset_collection()
    except Exception:
        store = Chroma(
            collection_name=name,
            embedding_function=embeddings,
            persist_directory="./chroma_db"
        )
        store.reset_collection()
    return store


# Search
tavily = TavilySearchResults(max_results=5)

# æ•°æ®ç»“æ„
class QuestionNode:
    def __init__(self, text: str, parent=None):
        self.text: str = text
        self.parent: Optional['QuestionNode'] = parent
        self.children: List['QuestionNode'] = []
        self.summary: Optional[str] = None
        self.is_researched: bool = False

# å·¥å…·å‡½æ•°
@traceable(name="Planner")
def planner(topic: str) -> List[str]:
    prompt = (
        f"ä¸»é¢˜: {topic}\n\n"
        "è¯·å°†è¿™ä¸ªä¸»é¢˜æ‹†è§£ä¸º3-5ä¸ªå…³é”®å­é—®é¢˜ï¼Œè¦†ç›–ç°çŠ¶ã€åº”ç”¨ã€æŠ€æœ¯ã€æŒ‘æˆ˜ã€è¶‹åŠ¿ã€‚\n"
        "è¾“å‡ºJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­é—®é¢˜ã€‚ä¸è¦å›ç­”æ— å…³å†…å®¹ã€‚"
    )
    resp = llm_call(prompt)
    try:
        sub_qs = json.loads(resp)
    except:
        sub_qs = [topic]
    return sub_qs

@traceable(name="Search Docs")
def search_docs(query: str) -> List[Dict]:
    docs: List[Dict] = tavily.invoke(query)
    if not docs: return []
    return [d for d in docs if isinstance(d, dict)]

@traceable(name="Summarizer Agent")
def summarize(query: str, docs: List[Dict]) -> str:
    joined = "\n\n".join(
        f"[{i+1}] {item.get('title','æ— æ ‡é¢˜')}\nURL: {item.get('url','No URL')}\n{item.get('content','')}"
        for i, item in enumerate(docs)
    )
    prompt = (
        f"å­é—®é¢˜: {query}\n\n"
        "è¯·æ€»ç»“ä»¥ä¸‹èµ„æ–™ä¸ºè¦ç‚¹ï¼ˆ<=6æ¡ï¼‰ï¼š\n\n"
        f"{joined}\n\nè¦ç‚¹ï¼š"
    )
    return llm_call(prompt)

@traceable(name="Critic Agent")
def critic_summary(query: str, summary: str) -> bool:
    prompt = (
        f"å­é—®é¢˜: {query}\n"
        f"ä»¥ä¸‹æ˜¯æ€»ç»“:\n{summary}\n\n"
        "è¯·æ£€æŸ¥æ€»ç»“æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦è¦†ç›–ä¸»è¦æ–¹é¢ï¼Œæ˜¯å¦å­˜åœ¨çŸ›ç›¾æˆ–é€»è¾‘é”™è¯¯ã€‚\n"
        "å›ç­” 'OK' è¡¨ç¤ºåˆæ ¼ï¼Œå¦åˆ™ 'NOT OK'ã€‚"
    )
    result = llm_call(prompt, temperature=0.0).strip().upper()
    return result == "OK"

def save_to_memory(vectorstore: Chroma, query: str, summary: str, topic: str):
    doc_id = str(uuid.uuid4())
    today = date.today().isoformat()
    content = f"[Query] {query} ({today})\n\n{summary}"
    vectorstore.add_texts([content], metadatas=[{"query": query, "date": today, "topic": topic}], ids=[doc_id])

def retrieve_memory(vectorstore: Chroma, query: str):
    docs = vectorstore.similarity_search(query, k=2)
    return docs

@traceable(name="Report Writer")
def write_report(topic: str, all_summaries: Dict[str,str]) -> str:
    joined = "\n\n".join(f"[{k}]\n{v}" for k,v in all_summaries.items())
    prompt = (
        f"ä¸»é¢˜: {topic}\n"
        f"å„å­é—®é¢˜æ€»ç»“:\n{joined}\n\n"
        "è¯·æ’°å†™å®Œæ•´ç ”ç©¶æŠ¥å‘Šï¼ˆ2000å­—å·¦å³)"
        "æŠ¥å‘Šè¦å¯»æ‰¾ä¸€äº›å¯é çš„å‚è€ƒï¼Œå¹¶ä¸”ä½¿ç”¨[1][2]æ ‡æ³¨å‡ºæ¥"
        "æ­£æ–‡ä¿ç•™å¼•ç”¨å‚è€ƒå¯¹åº”æ ‡å·[1][2]ã€‚\n"         #è¿™é‡Œå¯ä»¥è°ƒæ•´è¾“å‡ºæŠ¥å‘Šçš„å­—æ•°
        "ä½¿ç”¨çœŸå®æ¡ˆä¾‹å’Œæ•°æ®æ”¯æ’‘ï¼Œç»“æ„åŒ–å‘ˆç°ã€‚"
        "åªå¼•ç”¨æä¾›çš„ sourcesã€‚"
        "è‹¥æ— æ¥æºåˆ™æ˜ç¡®æ ‡æ³¨â€œæ— æ¥æºâ€ã€‚"
    )
    return llm_call(prompt, model="gpt-4o")

# æ ‘ä¸ç ”ç©¶èŠ‚ç‚¹
def build_tree(root_text: str, sub_questions: List[str]) -> QuestionNode:
    root_node = QuestionNode(root_text)
    for sq in sub_questions:
        root_node.children.append(QuestionNode(sq, parent=root_node))
    return root_node

def add_children_from_summary(node: QuestionNode):
    if node.children or not node.summary: return
    lines = [line.strip() for line in node.summary.split("\n") if line.strip()]
    for line in lines:
        if line[0].isdigit():
            node.children.append(QuestionNode(line, parent=node))

#  ç ”ç©¶èŠ‚ç‚¹ï¼ˆå¹¶è¡ŒåŒ–+é‡è¯•é™åˆ¶ï¼‰
MAX_RETRIES = 3

def research_node(vectorstore: Chroma, node: QuestionNode, topic: str):
    if node.is_researched: return
    mem_docs = retrieve_memory(vectorstore, node.text)
    if mem_docs:
        node.summary = "\n".join(d.page_content for d in mem_docs)
    else:
        docs = search_docs(node.text)
        node.summary = summarize(node.text, docs) if docs else "æœªæ‰¾åˆ°èµ„æ–™ã€‚"
        retries = 0
        while not critic_summary(node.text, node.summary) and retries < MAX_RETRIES:
            docs = search_docs(node.text)
            node.summary = summarize(node.text, docs)
            retries += 1
        save_to_memory(vectorstore, node.text, node.summary, topic)
    node.is_researched = True
    add_children_from_summary(node)

def research_nodes_parallel(vectorstore: Chroma, nodes: List[QuestionNode], topic: str):
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(research_node, vectorstore, node, topic) for node in nodes]
        for f in futures:
            f.result()

#  èœå•
def show_menu(node: QuestionNode, is_root=False) -> Dict:
    menu_text = f"\nğŸ“ å½“å‰èœå•: {node.text}\n"
    option_mapping = {}
    for idx, child in enumerate(node.children, 1):
        preview = child.summary.split("\n")[0] if child.summary else "âŒ Not researched"
        menu_text += f"{idx}. {child.text} | ğŸ” {preview}\n"
        option_mapping[idx] = ("child", child)
    extra_options = []
    if is_root:
        extra_options = [
            ("æ·»åŠ è‡ªå®šä¹‰é—®é¢˜", "custom"),
            ("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š", "generate_report"),
            ("é€€å‡º", "exit")
        ]
    else:
        extra_options = [("è¿”å›ä¸Šä¸€çº§", "back")]
    for j, (label, action) in enumerate(extra_options, len(node.children)+1):
        menu_text += f"{j}. {label}\n"
        option_mapping[j] = (action, None)
    return menu_text, option_mapping

def collect_summaries(node: QuestionNode) -> Dict[str,str]:
    summaries = {}
    def _collect(n: QuestionNode):
        if n.summary:
            summaries[n.text] = n.summary
        for c in n.children:
            _collect(c)
    _collect(node)
    return summaries

# ä¸»å¾ªç¯
if __name__ == "__main__":
    print("    Deep Research Agent (Enhanced)    ")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ").strip()
    vectorstore = get_vectorstore(topic)
    sub_questions = planner(topic)
    root_node = build_tree(topic, sub_questions)

    # å¹¶è¡Œç ”ç©¶æ‰€æœ‰å­é—®é¢˜
    research_nodes_parallel(vectorstore, root_node.children, topic)

    # èœå•äº¤äº’
    stack: List[QuestionNode] = [root_node]
    while stack:
        current_node = stack[-1]
        is_root = (current_node.parent is None)
        menu_text, option_mapping = show_menu(current_node, is_root=is_root)
        choice = input(menu_text + "è¯·è¾“å…¥ç¼–å·: ").strip()
        if not choice.isdigit():
            print("âš ï¸ è¯·è¾“å…¥æ•°å­—ç¼–å·ã€‚")
            continue
        choice = int(choice)
        if choice not in option_mapping:
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ã€‚")
            continue
        action, param = option_mapping[choice]

        if action == "child":
             node = param
             # å¦‚æœè¯¥å­é—®é¢˜è¿˜æ²¡æœ‰å­é—®é¢˜ï¼Œç”ŸæˆäºŒçº§å­é—®é¢˜
             if not node.children:
                 sub_questions = planner(node.text)  # ä»¥è¯¥å­é—®é¢˜ä¸ºä¸»é¢˜ç”Ÿæˆä¸‹ä¸€çº§å­é—®é¢˜
                 for sq in sub_questions:
                     node.children.append(QuestionNode(sq, parent=node))
                 # å¯¹äºŒçº§å­é—®é¢˜å¹¶è¡Œç ”ç©¶
                 research_nodes_parallel(vectorstore, node.children, topic)
             # æ ‡è®°å½“å‰èŠ‚ç‚¹å·²ç ”ç©¶
             research_node(vectorstore, node, topic)
             stack.append(node)

        elif action == "back":
            stack.pop()
        elif action == "custom":
            q = input("è¯·è¾“å…¥è‡ªå®šä¹‰é—®é¢˜: ").strip()
            new_node = QuestionNode(q, parent=current_node)
            current_node.children.append(new_node)
            research_node(vectorstore, new_node, topic)
        elif action == "generate_report":
            summaries = collect_summaries(root_node)
            report = write_report(topic, summaries)
            print("\nğŸ“˜ æœ€ç»ˆç ”ç©¶æŠ¥å‘Š:\n")
            print(report)
            with open("deep_research_report.txt","w", encoding="utf-8") as f:
                f.write(report)
            print("âœ… å·²ä¿å­˜ä¸º deep_research_report.txt")
        elif action == "exit":
            print("ğŸ‘‹ Exited.")
            break
