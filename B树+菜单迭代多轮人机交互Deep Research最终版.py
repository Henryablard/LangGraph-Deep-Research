#Bæ ‘+èœå•è¿­ä»£æœ€ç»ˆç‰ˆ
# coding: utf-8
import os
import uuid
import json
import hashlib
from typing import List, Dict, Optional
from datetime import date
from dotenv import load_dotenv

# å¤–éƒ¨ä¾èµ–ï¼ˆä½ å·²æœ‰çš„ç¯å¢ƒï¼‰
from openai import OpenAI
from langsmith.wrappers import wrap_openai
from langsmith import traceable
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults

# åˆå§‹åŒ–ï¼ˆè¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²é…ç½®ï¼‰
load_dotenv()
os.environ.setdefault("LANGSMITH_PROJECT", "deep_research_tracing")
os.environ.setdefault("LANGSMITH_TRACING", "true")

# åŒ…è£… OpenAIï¼ˆLangSmith è¿½è¸ªï¼‰
openai_client = wrap_openai(OpenAI())

# embeddings / search
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
tavily = TavilySearchResults(max_results=5)

# åŸºæœ¬æ•°æ®ç»“æ„
class QuestionNode:
    def __init__(self, text: str, parent: Optional["QuestionNode"] = None):
        self.text: str = text
        self.parent: Optional["QuestionNode"] = parent
        self.children: List["QuestionNode"] = []
        self.summary: Optional[str] = None
        self.is_researched: bool = False

# LLM å°è£…
@traceable(name="LLM Call")
def llm_call(prompt: str, model: str="gpt-4o-mini", temperature: float=0.5) -> str:
    """
    ç®€å•å°è£… LLM è°ƒç”¨ï¼Œè¿”å›å­—ç¬¦ä¸²ï¼ˆcontentï¼‰
    """
    resp = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )
    # å…¼å®¹ä¸åŒè¿”å›ç»“æ„
    try:
        return resp.choices[0].message.content
    except Exception:
        # fallback
        return str(resp)

# Vectorstore è¾…åŠ©ï¼ˆChromaï¼‰
def get_vectorstore(topic: str) -> Chroma:
    topic_hash = hashlib.md5(topic.encode("utf-8")).hexdigest()
    name = f"deep_research_memory_{topic_hash}"
    os.makedirs("./chroma_db", exist_ok=True)
    try:
        store = Chroma(
            collection_name=name,
            embedding_function=embeddings,
            persist_directory="./chroma_db"
        )
        # è‹¥éœ€è¦é‡ç½®ç­‰å¯åœ¨æ­¤å¤„ç†
    except Exception:
        store = Chroma(
            collection_name=name,
            embedding_function=embeddings,
            persist_directory="./chroma_db"
        )
    return store

def save_to_memory(vectorstore: Chroma, query: str, summary: str, topic: str):
    doc_id = str(uuid.uuid4())
    today = date.today().isoformat()
    content = f"[Query] {query} ({today})\n\n{summary}"
    try:
        vectorstore.add_texts([content], metadatas=[{"query": query, "date": today, "topic": topic}], ids=[doc_id])
    except Exception:
        pass

def retrieve_memory(vectorstore: Chroma, query: str):
    try:
        return vectorstore.similarity_search(query, k=2)
    except Exception:
        return []

#  Plannerï¼ˆç”Ÿæˆ n ä¸ªå­é—®é¢˜ï¼‰
@traceable(name="Planner")
def planner(topic: str, n: int = 3) -> List[str]:
    """
    å°è¯•è®© LLM è¿”å› JSON æ•°ç»„ï¼ˆä¸¥æ ¼ï¼‰ï¼Œè‹¥å¤±è´¥åˆ™å¯¹è¡Œåˆ†å‰²å¹¶æ¸…æ´—ã€‚
    è¿”å›å»é‡åçš„å­é—®é¢˜åˆ—è¡¨ï¼ˆæœ€å¤š n ä¸ªï¼‰ã€‚
    """
    prompt = (
        f"ä¸»é¢˜: {topic}\n\n"
        f"è¯·å°†è¿™ä¸ªä¸»é¢˜æ‹†è§£ä¸º {n} ä¸ªå…³é”®å­é—®é¢˜ï¼Œè¦†ç›–ç°çŠ¶ã€åº”ç”¨ã€æŠ€æœ¯ã€æŒ‘æˆ˜ã€è¶‹åŠ¿ã€‚\n"
        "åªè¿”å› JSON æ•°ç»„ï¼Œç¤ºä¾‹ï¼š [\"å­é—®é¢˜1\",\"å­é—®é¢˜2\",...]\n"
        "ä¸è¦è¾“å‡ºå…¶å®ƒè¯´æ˜ã€‚"
    )
    raw = llm_call(prompt, temperature=0.3)
    # è§£æ JSON ä¼˜å…ˆ
    items: List[str] = []
    try:
        parsed = json.loads(raw)
        if isinstance(parsed, list):
            items = [str(x).strip() for x in parsed]
    except Exception:
        # é€€åŒ–ä¸ºæŒ‰è¡Œè§£æï¼ˆå»ç¼–å·ï¼‰
        lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        for ln in lines:
            # å»æ‰å¼€å¤´ç¼–å·å¦‚ "1. " æˆ– "ï¼ˆ1ï¼‰"
            cleaned = ln.lstrip("0123456789.ï¼‰) ã€- ").strip()
            if cleaned:
                items.append(cleaned)
    # æ¸…æ´—ï¼šå»ç©ºã€å»é‡å¤ã€å»ä¸ topic ç›¸åŒçš„é¡¹
    cleaned: List[str] = []
    seen = set()
    topic_lower = topic.strip().lower()
    for it in items:
        it_s = it.strip()
        if not it_s:
            continue
        key = it_s.lower()
        if key in seen:
            continue
        # è·³è¿‡ä¸ topic æåº¦ç›¸ä¼¼çš„ç»“æœ
        if topic_lower in key or key in topic_lower:
            continue
        seen.add(key)
        cleaned.append(it_s)
        if len(cleaned) >= n:
            break

    # å¦‚æœ cleaned ä¸å¤Ÿï¼Œè¡¥é½å¸¸ç”¨æ¨¡æ¿ï¼ˆä¿è¯æœ‰ n ä¸ªï¼‰
    fallback_templates = [
        "ç°çŠ¶ä¸æ•°æ®æ¦‚è§ˆ",
        "æ ¸å¿ƒé©±åŠ¨å› ç´ /å½±å“å› ç´ ",
        "ä¸»è¦æŒ‘æˆ˜ä¸ç—›ç‚¹",
        "å…¸å‹æ¡ˆä¾‹ä¸å®è·µ",
        "æ”¿ç­–ä¸æ³•è§„å½±å“",
        "æœªæ¥è¶‹åŠ¿ä¸å±•æœ›",
        "æŠ€èƒ½/äººæ‰éœ€æ±‚åˆ†æ",
        "è¡Œä¸šåˆ†å¸ƒä¸æœºä¼š"
    ]
    idx = 0
    while len(cleaned) < n:
        cand = f"{fallback_templates[idx % len(fallback_templates)]}"
        # è‹¥é‡å¤åˆ™é€šè¿‡ç¼–å·å¾®è°ƒ
        candidate = cand if cand not in cleaned else f"{cand} {len(cleaned)+1}"
        if candidate not in cleaned:
            cleaned.append(candidate)
        idx += 1

    return cleaned[:n]

# æœç´¢ä¸æ‘˜è¦
@traceable(name="Search Docs")
def search_docs(query: str) -> List[Dict]:
    try:
        docs = tavily.invoke(query)
        return [d for d in docs if isinstance(d, dict)]
    except Exception:
        return []

@traceable(name="Summarizer Agent")
def summarize(query: str, docs: List[Dict]) -> str:
    # å°† docs æ‹¼æ¥æˆ prompt ç»™ LLM åšè¦ç‚¹æ€»ç»“
    joined = "\n\n".join(
        f"[{i+1}] {item.get('title','æ— æ ‡é¢˜')}\nURL: {item.get('url','No URL')}\n{item.get('content','')}"
        for i, item in enumerate(docs)
    )
    prompt = (
        f"å­é—®é¢˜: {query}\n\n"
        "è¯·åŸºäºä»¥ä¸‹èµ„æ–™æç‚¼ <=6 æ¡è¦ç‚¹ï¼ˆåˆ—å‡ºè¦ç‚¹ï¼Œæ¯æ¡ä¸è¶…è¿‡ä¸€è¡Œï¼‰ï¼š\n\n"
        f"{joined}\n\nè¦ç‚¹ï¼š"
    )
    resp = llm_call(prompt, temperature=0.2)
    return resp.strip()

@traceable(name="Critic Agent")
def critic_summary(query: str, summary: str) -> bool:
    prompt = (
        f"å­é—®é¢˜: {query}\n"
        f"ä»¥ä¸‹æ˜¯æ€»ç»“:\n{summary}\n\n"
        "è¯·æ£€æŸ¥æ€»ç»“æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦å­˜åœ¨çŸ›ç›¾ã€‚åªå›ç­” OK æˆ– NOT OKã€‚"
    )
    resp = llm_call(prompt, temperature=0.0).strip().upper()
    return resp == "OK"

# èŠ‚ç‚¹å±•å¼€ï¼ˆç ”ç©¶ + ç”Ÿæˆä¸‹ä¸€å±‚ï¼‰
def expand_node(node: QuestionNode, n: int, vectorstore: Chroma, topic: str):
    """
    å¯¹ node åš research å¹¶ç”Ÿæˆ n ä¸ªå­èŠ‚ç‚¹ï¼ˆä¸‹ä¸€å±‚ï¼‰ã€‚
    - ä¼šå…ˆå°è¯•ä» memory è¯»å– summary
    - è‹¥æ— ï¼Œåˆ™æ£€ç´¢å¹¶ summarize
    - è°ƒç”¨ planner ç”Ÿæˆå­é—®é¢˜ï¼Œè¿‡æ»¤é‡å¤/ä¸çˆ¶ç›¸åŒé¡¹ï¼›ä¸è¶³æ—¶è¡¥é½æ¨¡æ¿
    """
    if node.is_researched and node.children:
        return

    # research å½“å‰èŠ‚ç‚¹ï¼ˆsummaryï¼‰
    mem_docs = retrieve_memory(vectorstore, node.text)
    if mem_docs:
        try:
            node.summary = "\n".join(d.page_content for d in mem_docs)
        except Exception:
            node.summary = str(mem_docs)
    else:
        docs = search_docs(node.text)
        if docs:
            node.summary = summarize(node.text, docs)
        else:
            # å¦‚æœæ²¡æœ‰æœåˆ°èµ„æ–™ï¼Œæ ‡æ³¨ä¸ºæ— æ¥æº
            node.summary = "æœªæ‰¾åˆ°èµ„æ–™ã€‚"
        # è¯„å®¡ä¸é‡è¯•ï¼ˆç®€å•ä¸€æ¬¡ï¼‰
        try:
            if not critic_summary(node.text, node.summary):
                docs2 = search_docs(node.text)
                if docs2:
                    node.summary = summarize(node.text, docs2)
        except Exception:
            pass
        save_to_memory(vectorstore, node.text, node.summary, topic)

    node.is_researched = True

    # ç”Ÿæˆä¸‹ä¸€å±‚å­é—®é¢˜ï¼ˆä¸¥æ ¼ n ä¸ªï¼Œä¸”è¿‡æ»¤æ‰ä¸ node.text ç›¸åŒæˆ–é‡å¤é¡¹ï¼‰
    sub_qs = planner(node.text, n)
    # è¿‡æ»¤å’Œå»é‡
    filtered = []
    seen = set()
    node_lower = node.text.strip().lower()
    for s in sub_qs:
        s_str = s.strip()
        key = s_str.lower()
        if not s_str:
            continue
        if key in seen:
            continue
        # è·³è¿‡ä¸çˆ¶èŠ‚ç‚¹æ–‡æœ¬ç›¸åŒ/é«˜åº¦é‡åˆçš„é¡¹
        if node_lower == key or node_lower in key or key in node_lower:
            continue
        seen.add(key)
        filtered.append(s_str)
    # è¡¥é½ï¼ˆè‹¥ planner äº§å‡ºè¢«è¿‡æ»¤åæ•°é‡ < nï¼‰
    fallback_templates = [
        "ç°çŠ¶ä¸æ•°æ®æ¦‚è§ˆ",
        "æ ¸å¿ƒé©±åŠ¨å› ç´ /å½±å“å› ç´ ",
        "ä¸»è¦æŒ‘æˆ˜ä¸ç—›ç‚¹",
        "å…¸å‹æ¡ˆä¾‹ä¸å®è·µ",
        "æ”¿ç­–ä¸æ³•è§„å½±å“",
        "æœªæ¥è¶‹åŠ¿ä¸å±•æœ›",
        "æŠ€èƒ½/äººæ‰éœ€æ±‚åˆ†æ",
        "è¡Œä¸šåˆ†å¸ƒä¸æœºä¼š"
    ]
    idx = 0
    while len(filtered) < n:
        candidate = f"{fallback_templates[idx % len(fallback_templates)]}"
        # æ‹¼ä¸Šçˆ¶èŠ‚ç‚¹ä»¥é¿å…è¿‡äºé€šç”¨å¯¼è‡´æ­§ä¹‰
        candidate_full = f"{node.text} â€” {candidate}"
        if candidate_full not in filtered:
            filtered.append(candidate_full)
        idx += 1

    # åˆ›å»ºå­èŠ‚ç‚¹
    node.children = [QuestionNode(text=c, parent=node) for c in filtered[:n]]

# é€’å½’é¢„ç”Ÿæˆæ ‘ï¼ˆå¯é€‰ï¼Œä¿è¯è¿›å…¥æ—¶æœ‰ä¸‹ä¸€å±‚ï¼‰
def research_tree(node: QuestionNode, n: int, vectorstore: Chroma, topic: str, max_depth: int = 2, current_depth: int = 0):
    expand_node(node, n, vectorstore, topic)
    if current_depth >= max_depth:
        return
    for child in node.children:
        research_tree(child, n, vectorstore, topic, max_depth, current_depth + 1)

# æ”¶é›† summaries
def collect_summaries(node: QuestionNode) -> Dict[str, str]:
    results: Dict[str, str] = {}
    def _collect(n: QuestionNode):
        if n.summary:
            results[n.text] = n.summary
        for c in n.children:
            _collect(c)
    _collect(node)
    return results

# èœå•æ˜¾ç¤ºï¼ˆä¸»èœå• / èŠ‚ç‚¹èœå•ï¼‰
def show_menu(node: Optional[QuestionNode], is_main_menu: bool = False) -> (str, Dict[int, tuple]):
    """
    - å¦‚æœ is_main_menu=Trueï¼šæ˜¾ç¤ºä¸»èœå•æ“ä½œé¡¹
    - å¦åˆ™ï¼šæ˜¾ç¤º node.childrenï¼ˆä¸‹ä¸€å±‚ï¼‰
      è‹¥ node.children ä¸ºç©ºï¼šæ˜¾ç¤ºâ€œæŸ¥çœ‹æ€»ç»“ / è¿”å›ä¸Šä¸€çº§â€
    è¿”å› (menu_text, option_mapping)
    option_mapping: {index: (action, param)}
    actions: "user_questions","custom_main","generate_report","exit","child","summary","back"
    """
    menu_lines: List[str] = []
    option_mapping: Dict[int, tuple] = {}
    idx = 1

    if is_main_menu:
        menu_lines.append("ğŸ“ å½“å‰èœå•: ä¸»èœå•")
        menu_lines.append(f"{idx}. ç”¨æˆ·é—®é¢˜"); option_mapping[idx] = ("user_questions", None); idx += 1
        menu_lines.append(f"{idx}. æ–°æ·»åŠ çš„é—®é¢˜"); option_mapping[idx] = ("custom_main", None); idx += 1
        menu_lines.append(f"{idx}. å†™æœ€ç»ˆæŠ¥å‘Š"); option_mapping[idx] = ("generate_report", None); idx += 1
        menu_lines.append(f"{idx}. é€€å‡º"); option_mapping[idx] = ("exit", None); idx += 1
    else:
        menu_lines.append(f"ğŸ“ å½“å‰èœå•: {node.text}")
        if node.children:
            for child in node.children:
                menu_lines.append(f"{idx}. {child.text}")
                option_mapping[idx] = ("child", child)
                idx += 1
            # always allow return
            menu_lines.append(f"{idx}. è¿”å›ä¸Šä¸€çº§"); option_mapping[idx] = ("back", None); idx += 1
        else:
            # å¶å­èŠ‚ç‚¹ï¼šæŸ¥çœ‹ summary æˆ– è¿”å›
            menu_lines.append(f"{idx}. æŸ¥çœ‹æ€»ç»“"); option_mapping[idx] = ("summary", node); idx += 1
            menu_lines.append(f"{idx}. è¿”å›ä¸Šä¸€çº§"); option_mapping[idx] = ("back", None); idx += 1

    menu_text = "\n".join(menu_lines) + "\n"
    return menu_text, option_mapping

# ä¸»ç¨‹åº
def main_loop():
    print("    Deep Research Agent    ")
    topic = input("è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜: ").strip()
    n_subs_in = input("è¯·è¾“å…¥æ¯çº§ç”Ÿæˆå­é—®é¢˜æ•°é‡ nï¼ˆå»ºè®® 2~6ï¼‰: ").strip()
    n_subs = int(n_subs_in) if n_subs_in.isdigit() and int(n_subs_in) > 0 else 3
    max_depth_in = input("è¯·è¾“å…¥æ ‘æœ€å¤§æ·±åº¦ï¼ˆå»ºè®® 1~3ï¼‰: ").strip()
    max_depth = int(max_depth_in) if max_depth_in.isdigit() and int(max_depth_in) >= 0 else 2

    vectorstore = get_vectorstore(topic)
    root_node = QuestionNode(topic)

    # é¢„ç”Ÿæˆæ ‘ï¼ˆç¡®ä¿è¿›å…¥ã€Œç”¨æˆ·é—®é¢˜ã€æ—¶èƒ½ç›´æ¥æ˜¾ç¤ºç¬¬ä¸€çº§å­é—®é¢˜ï¼‰
    research_tree(root_node, n_subs, vectorstore, topic, max_depth=max_depth)

    stack: List[QuestionNode] = []
    in_main_menu = True

    while True:
        if in_main_menu:
            menu_text, option_mapping = show_menu(None, is_main_menu=True)
        else:
            current_node = stack[-1]
            # å¦‚æœæŸèŠ‚ç‚¹ children è¿˜æœªç”Ÿæˆï¼ˆä¸åº”è¯¥å‡ºç°ï¼‰ï¼Œå†ç¡®ä¿ç”Ÿæˆä¸€æ¬¡
            if not current_node.children and current_node.is_researched:
                expand_node(current_node, n_subs, vectorstore, topic)
            menu_text, option_mapping = show_menu(current_node, is_main_menu=False)

        choice = input(menu_text + "è¯·è¾“å…¥ç¼–å·: ").strip()
        if not choice.isdigit():
            print("âš ï¸ è¯·è¾“å…¥æ•°å­—ç¼–å·ã€‚")
            continue
        idx = int(choice)
        if idx not in option_mapping:
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ã€‚")
            continue

        action, param = option_mapping[idx]

        # ä¸»èœå•æ“ä½œ
        if in_main_menu:
            if action == "user_questions":
                # è¿›å…¥ä¸»é¢˜æ ‘ï¼ˆç¡®ä¿ root å·²ç»å±•å¼€ï¼‰
                if not root_node.children:
                    expand_node(root_node, n_subs, vectorstore, topic)
                stack.append(root_node)
                in_main_menu = False
            elif action == "custom_main":
                q = input("è¯·è¾“å…¥è‡ªå®šä¹‰é—®é¢˜ï¼ˆä½œä¸ºæ–°æ ¹ï¼‰ï¼š").strip()
                if not q:
                    print("âš ï¸ é—®é¢˜ä¸ºç©ºï¼Œå–æ¶ˆã€‚")
                    continue
                new_node = QuestionNode(q, parent=None)
                expand_node(new_node, n_subs, vectorstore, topic)
                stack.append(new_node)
                in_main_menu = False
            elif action == "generate_report":
                all_summaries = collect_summaries(root_node)
                report = write_report(topic, all_summaries)
                print("\nğŸ“˜ æœ€ç»ˆç ”ç©¶æŠ¥å‘Š:\n")
                print(report)
                with open("deep_research_report.txt", "w", encoding="utf-8") as f:
                    f.write(report)
                print("âœ… å·²ä¿å­˜ä¸º deep_research_report.txt")
            elif action == "exit":
                print("ğŸ‘‹ é€€å‡ºã€‚")
                break

        # æ ‘èŠ‚ç‚¹èœå•æ“ä½œ
        else:
            if action == "child":
                child_node: QuestionNode = param
                # è¿›å…¥ä¸‹ä¸€å±‚ï¼šç¡®ä¿ child çš„ä¸‹ä¸€å±‚å·²ç”Ÿæˆï¼ˆå¦‚æœä½ é¢„ç”Ÿæˆäº†æ ‘ï¼Œè¿™é‡Œå·²ç»æœ‰ childrenï¼‰
                if not child_node.children and child_node.is_researched:
                    expand_node(child_node, n_subs, vectorstore, topic)
                stack.append(child_node)
            elif action == "summary":
                node: QuestionNode = param
                # è‹¥å°šæœªç ”ç©¶ï¼Œå…ˆç ”ç©¶
                if not node.is_researched:
                    expand_node(node, n_subs, vectorstore, topic)
                print(f"\nğŸ“Œ {node.text} çš„æ€»ç»“:\n{node.summary}\n")
            elif action == "back":
                stack.pop()
                if len(stack) == 0:
                    in_main_menu = True

if __name__ == "__main__":
    main_loop()
